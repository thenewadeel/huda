---
title: "Project Al-Huda"
output: html_document
---

> In continuation from [Session 1](https://rpubs.com/glasschakotra/huda1)


```{r setup, include=FALSE}

#knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(jsonlite)
library(wordcloud2)
library(tm)
```

# Word Clouds

Now let's load up all of the text of **Al-Quran** and make a word cloud.
Wordclouds give a bird's eye view of any text.
The size of every word is dictated by the number of occurrences.
So the bigger the word, the more often it is mentioned in the text.

## 1. Loading Text

The `json` files I prepared earlier will be used now.

* `arabicAyahs.json`    -   All Arabic text of Al-Quran in Uthmani text (with aeraab)
* `arabicAyahsSimple.json`    -   All Arabic text of Al-Quran in simplified form (without aeraab)
* `englishAyahs.json`   -   English Translation (Saheeh International Version)

```{r include=TRUE}
Arabic <- fromJSON("arabicAyahsSimple.json")$ayahs
ArabicUthmani <- fromJSON("arabicAyahs.json")$ayahs
English <- fromJSON("englishAyahs.json")$ayahs
# The required text is encapsulated in the `ayahs` object within every `json` file.

Arabic$text[1]
ArabicUthmani$text[1]
English$text[1]

```
## 2. Preparing Word-Frequency Dataframes

This part is a bit hectic.
It involves the following steps:

1.    Preparing a Corpus
2.    Generating a Term-Document Matrix
3.    Composing a Data-Frame with word-freq variables

All 3 types of inputs are handled separately now.
```{r out.width="100%"}
Arabic<-filter(Arabic,surat_id>89)
ArabicUthmani<-filter(ArabicUthmani,surat_id>89)
English<-filter(English,surat_id>89)

Arabic.corpus <- Corpus(VectorSource(Arabic$text),
                       readerControl = list(language = "ar"))
ArabicUthmani.corpus <- Corpus(VectorSource(ArabicUthmani$text),
                              readerControl = list(language = "ar"))
English.corpus <- Corpus(VectorSource(English$text),
                        readerControl = list(language = "en"))
```

The 3 corpus objects now contain the required corpora.
The English corpus needs a bit of work to remove certain insignificant words.

```{r message=FALSE, warning=FALSE}

English.corpus = English.corpus %>%
  tm_map(removePunctuation) %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(removeWords, c("and","but","then","they")) %>%
  tm_map(removeWords, stopwords("SMART")) 
```

The corpora are now ready and can be used to generate a Term-Document matrix.

```{r}
tdm = TermDocumentMatrix(Arabic.corpus) %>%
  as.matrix()
```

This tdm can now be used to make a dataframe with word-frequency data.

```{r}
words = sort(rowSums(tdm), decreasing = TRUE)
df = data.frame(word = names(words), freq = words)

```

## 3. Making The WordCloud

```{r out.width="100%"}
#https://fromsystosys.netlify.app/2019/03/03/color-palettes-inspired-by-islamic-art/
#devtools::install_github("lambdamoses/IslamicArt")

cloud.colors = c("#301d25", "#7a4d41", "#275238","#9c1839","#6ab668","#59d0ee","#6767f4","#d6273b","#fffb49","#e69975","#5a4239","#8d332a")
cloud.background = "#d8cdbe"

library(extrafont)
# font_import()
#fonts()

wordcloud2(df,
           color = rep_len(cloud.colors, nrow(df)),
           backgroundColor = cloud.background,
           fontFamily = "Titr",
           size = 2,
           minSize = 5,
           rotateRatio = 0)

```